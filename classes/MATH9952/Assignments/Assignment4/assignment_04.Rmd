---
title: "Building a Tree Model with the Retention Dataset"
author: "Jerry Kiely"
date: "11 April 2017"
header-includes:
    - \usepackage{setspace}\doublespacing
    # - \usepackage{setspace}\onehalfspacing
    - \usepackage{float}
output: 
  pdf_document:
    fig_caption: yes
    # toc: yes
---


```{r echo=FALSE, message=FALSE}


set.seed(27041970)


library(rpart)
library(ROCR)
library(knitr)

retention          = read.csv("~/Workspace/College/DIT/MATH9952/Data/students_retention_full.csv", header = T)
outcome            = c('failed', 'passed')
retention$result   = factor(retention$overall6, levels = 0:1, labels = outcome)
retention$overall6 = NULL
retention$id       = NULL

attach(retention)

include    = rbinom(dim(retention)[1], 1, 2/3)
training   = subset(retention, include == 1)
validation = subset(retention, include == 0)

```


## Introduction


This analysis is concerned with data relating to student retention in the Engineering 
faculty of DIT. It is the same data considered in Assignment 3, but with more potential 
predictors available.

The purpose of the analysis will be to use rpart models to build a predictive model 
for student retention. The data includes risk risk factors regarding prior academic 
performance (e.g. leaving certificate results, leaving certificate maths grade), 
personal characteristics (gender, home address, CAO choices made etc).


## The Algorithm

rpart takes a data set and recursively partitions it based on a splitting criteria - 
choosing the split that maximizes the reduction in impurity for the node. For example 
in the case of the root, and splitting on lcpoints, deciding where to split within 
lcpoints involves:

 * iterating over all values of lcpoints in turn
 * calculating the reduction in impurity for splitting at each value
 * keeping track of the value that results in the maximum impurity reduction
 * splitting at the value that results in the maximum imppurity reduction

this process is repeated at every level until other criteria such as maxdepth or 
minsplit come in to play. The impurity of a node can be calculated using a gini 
score or an entropy score. An example of this process is given in the code - specifically 
finding the splitting point for the lcpoints predictor using a gini score.


## The Data


First we read the data in. For the sake of readibility we will add a column called "result"" that 
will hold a string value - "failed" or "passed". Also we will drop the "id" column as unnecessary, 
and the "overall6" column as redundant. Next we split the data into a training and validation set. 
The training set will be used to create our model, and the validation set will be used to see how 
well it performs, and to help with the process of improving it through pruning.


```{r tree-full, echo=FALSE, fig.cap="\\label{tree-full}Tree built with default parameters and cp = 0"}

fit0 = rpart(result ~ ., data = training, method = 'class', cp = -0)
cp1  = fit0$cptable[which.min(fit0$cptable[, "xerror"]), "CP"]

plot(fit0, margin = 0.01, uniform = T)
text(fit0, use.n = T, xpd = T, minlength = 6, cex = 0.6)

```


## The Model


We fit a model with all variables included as potential predictors. The default values are used for 
all parameters except for the complexity parameter, which is set to 0. The plot of the tree for this 
model is included in \autoref{tree-full}.


```{r cp-cv, echo=FALSE, fig.cap="\\label{cp-cv}Complexity parameters versus cross validation results versus size of tree"}

plotcp(fit0, cex = 0.6)

```


In \autoref{cp-cv} we see a plot of the complexity parameters for the tree. The value of 
interest is the left-most value below the dotted line:

 * below the dotted line because we are interested in the complexity parameter with the the 
    lowest cross validation error
 * left-most because we favour a smaller tree over a deeper, more complex, tree which could 
    suffer from overfitting

so we choose a value of `r cp1 ` for our complexity parameter, and prune our tree based on that.  


```{r echo=FALSE}

kable(fit0$cptable, caption = "\\label{tb-cp}table detailing complexity parameters")

```


In \autoref{tree-pruned} we see our pruned tree. As can be seen it is a lot less complex than 
the earlier version.


```{r echo=FALSE, fig.cap="\\label{tree-pruned}Tree pruned with found complexity parameter"}

fit1 = prune(fit0, cp = cp1)

plot(fit1, margin = 0.01, uniform = T)
text(fit1, use.n = T, xpd = T, minlength = 6, cex = 0.7)

```


## The Analysis


Now we can analyze the results of our pruning. We will now predict based on our validatition set 
to see how well our model performs. 


```{r echo=FALSE}

pred.labels = rev(outcome)

prediction  = predict(fit1, newdata = validation)

outcome     = rep(pred.labels[1], nrow(prediction))
outcome[prediction[, 1] > 0.5] = pred.labels[2]

kable(prop.table(xtabs(~ validation$result + outcome), 1), caption = "\\label{tb-cm}Confusion matrix for pruned tree")

```


Looking at the confusion matrix in \autoref{tb-cm} we seem to do a better job of predicting passing 
than predicting failing - i.e. the true failed is below 0.5 and the true passed well above 0.5. An 
option might be to consider treating the model as a predictor of successfully passing rather than failing.


```{r echo=FALSE, fig.cap="\\label{roc-curve}ROC curve with AUC value"}

p.scores = prediction(prediction[,1], validation$result, label.ordering = pred.labels)
p.perf   = performance(p.scores, "tpr", "fpr")
p.auc    = performance(p.scores, "auc")

plot(p.perf, col = "blue", lwd = 2, xlab = '1-Specificity (FPR)', ylab = 'Sensitivity (TPR)')
abline(a = 0, b = 1, col = 'red', lty = 3, lwd = 2)
text(0.1, 1, round(p.auc@y.values[[1]], 3), col = 'red', cex = 0.6)

```


We will next look at an ROC curve for the model in \autoref{roc-curve}. The blue curve gives an indication 
of how much better than random guessing our model is, with the red dotted line representing the random guess. 
The value in red is the AUC, or the area under the curve, for the ROC curve. A better than random 
model would have an AUC of > 0.5 (the area under the red dotted line).  


